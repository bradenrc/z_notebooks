{
  "paragraphs": [
    {
      "text": "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.storage.StorageLevel\nimport scala.io.Source\nimport scala.collection.mutable.HashMap\nimport java.io.File\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport sys.process.stringSeqToProcess\n\n/** Configures the Oauth Credentials for accessing Twitter */\ndef configureTwitterCredentials(apiKey: String, apiSecret: String, accessToken: String, accessTokenSecret: String) {\n  val configs \u003d new HashMap[String, String] ++\u003d Seq(\n    \"apiKey\" -\u003e apiKey, \"apiSecret\" -\u003e apiSecret, \"accessToken\" -\u003e accessToken, \"accessTokenSecret\" -\u003e accessTokenSecret)\n  println(\"Configuring Twitter OAuth\")\n  configs.foreach{ case(key, value) \u003d\u003e\n    if (value.trim.isEmpty) {\n      throw new Exception(\"Error setting authentication - value for \" + key + \" not set\")\n    }\n    val fullKey \u003d \"twitter4j.oauth.\" + key.replace(\"api\", \"consumer\")\n    System.setProperty(fullKey, value.trim)\n    println(\"\\tProperty \" + fullKey + \" set as [\" + value.trim + \"]\")\n  }\n  println()\n}\n\n// Configure Twitter credentials\nval apiKey \u003d \"h9ZHlkz53jYPWFoxSZNoFtuf9\"\nval apiSecret \u003d \"I4DEEOiNbR1OTO0UM5j5OsyLv7mcFAOySu6U4OHuVg85T0CpFm\"\nval accessToken \u003d \"23088031-k7lxpDWd9o5lcqrnkjZfbNsVNepGoZmNR6AcbrpJ4\"\nval accessTokenSecret \u003d \"TDbgAuKjfA2hjnq4I0cWWI16nNqq44vQOGHt10DUZhjpt\"\n\nconfigureTwitterCredentials(apiKey, apiSecret, accessToken, accessTokenSecret)\n\nimport org.apache.spark.streaming.twitter._\nval ssc \u003d new StreamingContext(sc, Seconds(2))\nval tweets \u003d TwitterUtils.createStream(ssc, None)\nval twt \u003d tweets.window(Seconds(60))\n\ncase class Tweet(createdAt:Long, text:String)\ntwt.map(status\u003d\u003e\n  Tweet(status.getCreatedAt().getTime()/1000, status.getText())\n).foreachRDD(rdd\u003d\u003e\n  // Below line works only in spark 1.3.0.\n  // For spark 1.1.x and spark 1.2.x,\n  // use rdd.registerTempTable(\"tweets\") instead.\n  rdd.toDF().registerTempTable(\"tweets\")\n)\n\ntwt.print\n\nssc.start()\n",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1438641858961_960355678",
      "id": "20150803-154418_1338059835",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.storage.StorageLevel\nimport scala.io.Source\nimport scala.collection.mutable.HashMap\nimport java.io.File\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport sys.process.stringSeqToProcess\nconfigureTwitterCredentials: (apiKey: String, apiSecret: String, accessToken: String, accessTokenSecret: String)Unit\napiKey: String \u003d h9ZHlkz53jYPWFoxSZNoFtuf9\napiSecret: String \u003d I4DEEOiNbR1OTO0UM5j5OsyLv7mcFAOySu6U4OHuVg85T0CpFm\naccessToken: String \u003d 23088031-k7lxpDWd9o5lcqrnkjZfbNsVNepGoZmNR6AcbrpJ4\naccessTokenSecret: String \u003d TDbgAuKjfA2hjnq4I0cWWI16nNqq44vQOGHt10DUZhjpt\nConfiguring Twitter OAuth\n\tProperty twitter4j.oauth.consumerKey set as [h9ZHlkz53jYPWFoxSZNoFtuf9]\n\tProperty twitter4j.oauth.accessToken set as [23088031-k7lxpDWd9o5lcqrnkjZfbNsVNepGoZmNR6AcbrpJ4]\n\tProperty twitter4j.oauth.consumerSecret set as [I4DEEOiNbR1OTO0UM5j5OsyLv7mcFAOySu6U4OHuVg85T0CpFm]\n\tProperty twitter4j.oauth.accessTokenSecret set as [TDbgAuKjfA2hjnq4I0cWWI16nNqq44vQOGHt10DUZhjpt]\n\nimport org.apache.spark.streaming.twitter._\nssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@51c7303b\ntweets: org.apache.spark.streaming.dstream.ReceiverInputDStream[twitter4j.Status] \u003d org.apache.spark.streaming.twitter.TwitterInputDStream@3f33f3a3\ntwt: org.apache.spark.streaming.dstream.DStream[twitter4j.Status] \u003d org.apache.spark.streaming.dstream.WindowedDStream@1c9813a9\ndefined class Tweet\n"
      },
      "dateCreated": "Aug 3, 2015 3:44:18 PM",
      "dateStarted": "Sep 14, 2015 11:00:17 AM",
      "dateFinished": "Sep 14, 2015 11:00:21 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect count(*)*  from tweets\n",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "createdAt",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "text",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "createdAt",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "text",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1442253537171_1709774105",
      "id": "20150914-105857_880694330",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.lang.RuntimeException: [2.19] failure: identifier expected\n\nselect count(*)*  from tweets\n\n                  ^\n\tat scala.sys.package$.error(package.scala:27)\n\tat org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:36)\n\tat org.apache.spark.sql.catalyst.DefaultParserDialect.parse(ParserDialect.scala:67)\n\tat org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:145)\n\tat org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:145)\n\tat org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:95)\n\tat org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:94)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)\n\tat scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)\n\tat org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)\n\tat org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:142)\n\tat org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:142)\n\tat org.apache.spark.sql.sources.DDLParser.parse(ddl.scala:48)\n\tat org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:165)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:744)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:133)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Sep 14, 2015 10:58:57 AM",
      "dateStarted": "Sep 14, 2015 11:32:41 AM",
      "dateFinished": "Sep 14, 2015 11:32:41 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1442253649851_1949608943",
      "id": "20150914-110049_1348890474",
      "dateCreated": "Sep 14, 2015 11:00:49 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "streaming twitter",
  "id": "2AV14ANQZ",
  "angularObjects": {
    "2ATDB6DPP": [],
    "2ATX2UP3E": [],
    "2ATJEEV3Z": [],
    "2AUT5SWNG": [],
    "2AV9JBN8B": [],
    "2AUH78YGG": [],
    "2AVK6ZW5M": [],
    "2AW72AXBP": [],
    "2AUV8UA7U": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}